**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#: 22       |  |
|-----------------|---|
| Student Names:  |  Topan Budiman |
|                 |  Mark Ngu |
|                 |  Jacob Schon |
|                 |  Muhammed Shah |


# Introduction

The purpose of this lab is to analyze test data using reliability assessment tools using the following two methods: Reliability Growth Testing and Reliability Demonstration Chart (RDC). We will perform tests on a hypothetical system using SRTAT (Software Reliability Testing Analysis Tool) to measure the failure rate, MTTF and reliability of the SUT. The RDC will tell us if the failure rate or MTTF is met.

# Assessment Using Reliability Growth Testing 

![image](https://user-images.githubusercontent.com/65151396/229208958-d5644f1b-07ca-42d1-8de0-6e70d881ef82.png)

**Figure 1:** Plot of Time Between Failure

The graph above is the plot of time between failure. When using the Littlewood and **Varral's Bayesian Relaibility model** to model the graph, we see that we get an exponential curve which is what we expected. This is ideally what we want because as the failure number increases, we want the time between each failure to also increase. Since it’s exponential, that means that the time between each failure increases greatly when compared to a linear plot. The acceptable range for time between failure would be the area below the curve. If we were to use an acceptable value for time between failure, we would select where the curve intercepts the Y-axis which is approximately at **600-700** units of time.

![image](https://user-images.githubusercontent.com/65151396/229209237-7371a4c8-1ad8-492c-bd5e-4813aa06d4d5.png)
**Figure 2:** Error message when trying to plot failure count/failure intensity

Although we weren’t able to plot the graph for failure intensity, the output we would expect is an inverse exponential graph. This is because per every unit of time that goes by, we would want the number of failures to decrease. Ideally, this graph would be exponential as this would show that the amount of failures per unit of time decreases at a much faster rate than a linear plot. The acceptable range of failure rate would be the area below the curve. 

# Assessment Using Reliability Demonstration Chart 

In the Reliability Demonstration Chart we had to make some assumptions including,
- Input events can be represented by seconds to use the time units instead of time units
- Our number of failures per call was assumed to be the number of failures per 24 hours and then we used seconds instead of hours as our time variable.
- The X axis on the RDC is the Number of Seconds in normalized units.
- We assumed that our FIO (Failure Intensity Objective) would be 12 failures per 86400 seconds, the chart says calls but we changed this unit to seconds.

<img width="700" alt="Screenshot 2023-04-05 at 6 41 46 PM" src="https://user-images.githubusercontent.com/65151396/230244270-7114194e-0f15-4df7-82c1-95cde2295147.png">
When changing the Discrimination Ratio (γ), Developer’s Risk (α) and User’s risk (β) we weren’t able to notice any observable difference in the RDC. It is quite possible that the values in the Risk-Trade-Off-Parameters were not linked to the generation of the graph as the values we had changed for γ, α, and β had no effect. Regardless, we were still able to experiment with the MTTF value in order to achieve the minimum value that would produce the closest result that would align with the plotted line between the continue test area and the acceptance area. After we achieved the correct MTTFmin (2 hours/7200 seconds) we testest what multiples of the minimum would produce (x2 & ½). 

MTTFmin is 2 hours or 7200 seconds, when using this MTTF in the RDC we achieve the following output:
<img width="719" alt="Screenshot 2023-04-05 at 6 42 35 PM" src="https://user-images.githubusercontent.com/65151396/230244529-5580c4d0-8a35-4640-a8a3-cf5b565024e0.png">

**Figure 3:** Plot Failure Number per Number of Seconds in a Normalized Usage Units (MTTFmin)

Upon doubling the MTTFmin our RDC appeared to have the observed failures be well in the acceptance zone as shown below:
<img width="711" alt="Screenshot 2023-04-05 at 6 43 30 PM" src="https://user-images.githubusercontent.com/65151396/230244621-4e021590-bcab-4a82-8b22-91e1984ed40f.png">

**Figure 4:** Plot Failure Number per Number of Seconds in a Normalized Usage Units (MTTFmin x2)

In contrast taking half of the MTTFmin, where the mean time to failure is expected to be once every 4 hours and the acceptance amount of failures every 24 hours is 6 we can observe that our RDC indicates that more testing is required as follows:


# Comparison of Results

The purpose of Reliability Growth Testing in part 1 is to analyze given failure data in order to find the failure rate, MTTF and reliability of an SUT. By using SRTAT we are able to plot a time over failure number graph to analyze the data. In part 2 we assess whether or not more testing is required using the Reliability Demonstration Chart. With a given MTTF and failure data, the excel sheet plots a failure number over the number of input/time units chart. Using this chart, we can determine the adequacy of the test suite depending on whether the plot lies within the accept, continue or reject areas of the chart. We also tested half the MTTF and double the MTTF to see how the chart would behave.

# Discussion on Similarity and Differences of the Two Techniques

With Reliability Growth Testing, the goal is to observe the SUT under stress over a long period of time. This is done to see how the system behaves over time and this will allow the tester to pinpoint specific model and design weaknesses. Two of the key metrics when performing reliability growth testing are Time between failure and Failure Intensity. Time between failure is used to measure the time interval between the current failure and the previous failure. Failure intensity is a measure of failures that occurred at a certain period of time.

RDC testing is used to determine whether the acceptable number of failures per a large amount of calls or time is being met based on the failure. The RDC program with excel sheets was limited to using the cumulative number  of “normal” input events and had their cells set up to produce calculations using the failures/calls as the nominal x-values on the graph. The RDC consists of three different sections which are plotted using an equation with 3 different variable constants that are set by the tester. The Discrimination Ratio, Developer’s Risk, and User’s Risk, which will determine where the Reject, Continue Test, and Accept plot lines are located. The Reject region is represented as the red sections, the Continue Test is the yellow section and the accept region is the one in green.

# How the team work/effort was divided and managed

Since the assignment consisted of two parts, we split into groups of two to complete each section of the assignment. Part one required running a program that could only run on Windows, so Muhammed and Topan took this task one since Muhammed had a Windows computer. Part two was analyzing the results from an Excel file so Gabe and Jacob worked on that section.

# Difficulties encountered, challenges overcome, and lessons learned

A difficulty we encountered during this assignment was that the program required for part one only ran on Windows. This drastically slowed down our workflow since only certain people could work on one section.

# Comments/feedback on the lab itself

This lab was very difficult to do since setting up the environment was very buggy. The program in part one only runs on windows and only one member of our group was able to work on it. 
